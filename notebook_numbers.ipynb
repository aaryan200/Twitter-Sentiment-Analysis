{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 23:35:42.786542: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-12 23:35:42.833341: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-12 23:35:42.833382: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-12 23:35:42.834461: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-12 23:35:42.840644: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-12 23:35:42.841456: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-12 23:35:44.208673: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/dcf-02/.local/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout, Activation, Input\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0            1         2  \\\n",
       "0  2401  Borderlands  Positive   \n",
       "1  2401  Borderlands  Positive   \n",
       "2  2401  Borderlands  Positive   \n",
       "3  2401  Borderlands  Positive   \n",
       "4  2401  Borderlands  Positive   \n",
       "\n",
       "                                                   3  \n",
       "0  im getting on borderlands and i will murder yo...  \n",
       "1  I am coming to the borders and I will kill you...  \n",
       "2  im getting on borderlands and i will kill you ...  \n",
       "3  im coming on borderlands and i will murder you...  \n",
       "4  im getting on borderlands 2 and i will murder ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df_train = pd.read_csv('data/twitter_training.csv', header=None)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_idx = {'Irrelevant': 0, 'Negative': -1, 'Neutral': 0, 'Positive': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82367"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find out number of different words in training set\n",
    "words = set()\n",
    "for sentence in df_train[3]:\n",
    "    for word in str(sentence).split():\n",
    "        words.add(word)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the above approach doesn't seem to be promising, continue with embeddings from glove-50\n",
    "def read_glove_file(filename = 'glove.6B.50d.txt'):\n",
    "    with open(filename, 'r') as f:\n",
    "        words = set()\n",
    "        words_to_vec_map = dict()\n",
    "        for line in f:\n",
    "            # Remove extra white spaces and split the line\n",
    "            li = line.strip().split()\n",
    "            words.add(li[0])\n",
    "            words_to_vec_map[li[0]] = np.array(li[1:], dtype=np.float64)\n",
    "    idx_to_words = dict()\n",
    "    words_to_idx = dict()\n",
    "    i = 1\n",
    "    for word in sorted(words):\n",
    "        words_to_idx[word] = i\n",
    "        idx_to_words[i] = word\n",
    "        i += 1\n",
    "    return words_to_vec_map, words_to_idx, idx_to_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_vec_map, words_to_idx, idx_to_words = read_glove_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxLen = len(max(df_train[3], key=lambda x:len(str(x).strip().split())).strip().split())\n",
    "maxLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(label_to_idx, csv_file = 'data/twitter_training.csv'):\n",
    "    X = []\n",
    "    y = []\n",
    "    with open(csv_file, 'r') as f:\n",
    "        csvReader = csv.reader(f)\n",
    "        for row in csvReader:\n",
    "            X.append(row[3])\n",
    "            y.append(label_to_idx[row[2]])\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y, dtype=int)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_data(label_to_idx, 'data/twitter_training.csv')\n",
    "X_val, y_val = load_data(label_to_idx, 'data/twitter_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_indices(X, words_to_idx ,maxLen):\n",
    "    m = X.shape[0]\n",
    "    X_out = np.zeros((m, maxLen))\n",
    "    for i in range(m):\n",
    "        li = X[i].lower().strip().split()\n",
    "        j = 0\n",
    "        for w in li:\n",
    "            if (j >= maxLen): break\n",
    "            if w in words_to_idx.keys():\n",
    "                X_out[i,j] = words_to_idx[w]\n",
    "            j += 1\n",
    "    return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_size = len(word_to_index) + 1              # adding 1 to fit Keras embedding (requirement)\n",
    "    any_word = list(word_to_vec_map.keys())[0]\n",
    "    emb_dim = word_to_vec_map[any_word].shape[0]    # define dimensionality of your GloVe word vectors (= 50)\n",
    "      \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1\n",
    "    # Initialize the embedding matrix as a numpy array of zeros.\n",
    "    # See instructions above to choose the correct shape.\n",
    "    emb_matrix = np.zeros((vocab_size, emb_dim))\n",
    "    \n",
    "    # Step 2\n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        emb_matrix[idx, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Step 3\n",
    "    # Define Keras embedding layer with the correct input and output sizes\n",
    "    # Make it non-trainable.\n",
    "    embedding_layer = Embedding(input_dim = vocab_size, output_dim = emb_dim, trainable = False)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Step 4 (already done for you; please do not modify)\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. \n",
    "    embedding_layer.build((None,)) # Do not modify the \"None\".  This line of code is complete as-is.\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentence_to_indices(X_train, words_to_idx, maxLen)\n",
    "X_val_indices = sentence_to_indices(X_val, words_to_idx, maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, words_to_vec_map, words_to_idx):\n",
    "    sentence_indices = Input(shape = input_shape)\n",
    "\n",
    "    embedding_layer = pretrained_embedding_layer(words_to_vec_map, words_to_idx)\n",
    "\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "\n",
    "    X = LSTM(units=128, return_sequences=True)(embeddings)\n",
    "\n",
    "    X = Dropout(rate = 0.5)(X)\n",
    "\n",
    "    X = LSTM(units = 128, return_sequences=False)(X)\n",
    "\n",
    "    X = Dropout(rate = 0.5)(X)\n",
    "\n",
    "    X = Dense(units= 1)(X)\n",
    "\n",
    "    X = Activation('tanh')(X)\n",
    "\n",
    "    model = Model(inputs = sentence_indices, outputs = X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model((maxLen, ), words_to_vec_map, words_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer= 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2334/2334 [==============================] - 242s 104ms/step - loss: 0.1163 - accuracy: 0.6327\n",
      "Epoch 2/30\n",
      "2334/2334 [==============================] - 250s 107ms/step - loss: 0.1116 - accuracy: 0.6350\n",
      "Epoch 3/30\n",
      "2334/2334 [==============================] - 247s 106ms/step - loss: 0.1091 - accuracy: 0.6372\n",
      "Epoch 4/30\n",
      "2334/2334 [==============================] - 244s 105ms/step - loss: 0.1069 - accuracy: 0.6380\n",
      "Epoch 5/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.1027 - accuracy: 0.6410\n",
      "Epoch 6/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0994 - accuracy: 0.6440\n",
      "Epoch 7/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0988 - accuracy: 0.6438\n",
      "Epoch 8/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0944 - accuracy: 0.6468\n",
      "Epoch 9/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0926 - accuracy: 0.6466\n",
      "Epoch 10/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0929 - accuracy: 0.6473\n",
      "Epoch 11/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0885 - accuracy: 0.6502\n",
      "Epoch 12/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0887 - accuracy: 0.6498\n",
      "Epoch 13/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0869 - accuracy: 0.6508\n",
      "Epoch 14/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0855 - accuracy: 0.6521\n",
      "Epoch 15/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0830 - accuracy: 0.6535\n",
      "Epoch 16/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0832 - accuracy: 0.6530\n",
      "Epoch 17/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0806 - accuracy: 0.6548\n",
      "Epoch 18/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0807 - accuracy: 0.6546\n",
      "Epoch 19/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0807 - accuracy: 0.6543\n",
      "Epoch 20/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0782 - accuracy: 0.6555\n",
      "Epoch 21/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0784 - accuracy: 0.6556\n",
      "Epoch 22/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0750 - accuracy: 0.6577\n",
      "Epoch 23/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0761 - accuracy: 0.6569\n",
      "Epoch 24/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0755 - accuracy: 0.6575\n",
      "Epoch 25/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0744 - accuracy: 0.6572\n",
      "Epoch 26/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0734 - accuracy: 0.6584\n",
      "Epoch 27/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0730 - accuracy: 0.6588\n",
      "Epoch 28/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0720 - accuracy: 0.6596\n",
      "Epoch 29/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0726 - accuracy: 0.6598\n",
      "Epoch 30/30\n",
      "2334/2334 [==============================] - 244s 104ms/step - loss: 0.0711 - accuracy: 0.6599\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_indices, y_train, epochs = 30, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 39ms/step - loss: 0.0562 - accuracy: 0.7000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.056153979152441025, 0.699999988079071]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val_indices, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dcf-02/miniconda3/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"model_numbers.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/32 [..............................] - ETA: 1s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 38ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_val = model.predict(X_val_indices)\n",
    "for idx, res in enumerate(y_pred_val):\n",
    "    temp = np.argmax(res)\n",
    "    # print(idx, res)\n",
    "    if (temp ==0): continue\n",
    "    elif (temp == 1): print(f\"Predicted 'Negative' for '{X_val[idx]}'\")\n",
    "    else: print(f\"Predicted 'Positive' for '{X_val[idx]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
